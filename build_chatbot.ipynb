{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabaudian/Conversational_Agents_project/blob/main/build_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB2PffeknEPI"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RikF5T34n7_5"
      },
      "outputs": [],
      "source": [
        "# === Install === #\n",
        "!pip install -q torch\n",
        "!pip install -q bert_score\n",
        "!pip install -q rouge_score\n",
        "!pip install -q transformers\n",
        "!pip install -q huggingface_hub[hf_xet]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "brlMWTqWti0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d232a798-735a-4b8b-f7a4-a642cd327a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIBRARY: \t| VERSION:\n",
            "> PyTorch\t| 2.6.0+cu124\n",
            "> Transformers\t| 4.51.3\n"
          ]
        }
      ],
      "source": [
        "# === Version Controls === #\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"LIBRARY: \\t| VERSION:\")\n",
        "print(f\"> PyTorch\\t| {torch.__version__}\") # 2.6.0+cu124\n",
        "print(f\"> Transformers\\t| {transformers.__version__}\") # 4.51.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqulJC75kucj"
      },
      "outputs": [],
      "source": [
        "# === Import === #\n",
        "\n",
        "# General\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import chain\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from bert_score import score\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd1M3t4kk9kK"
      },
      "outputs": [],
      "source": [
        "# === Constants & Set-Up === #\n",
        "\n",
        "# Google Drive Paths\n",
        "drive.mount(\"/content/drive\")                              # Mount Google Drive\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/Colab_Notebooks\"  # Path to Drive main directory\n",
        "\n",
        "# Directories & Paths\n",
        "DATA_DIR = os.path.join(DRIVE_BASE_DIR, \"data\")            # Path to data directory\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE_DIR, \"output\")        # Path to output directory (training result)\n",
        "PLOT_DIR = os.path.join(DRIVE_BASE_DIR, \"plots\")           # Path to plots directory\n",
        "METRICS_DIR = os.path.join(DATA_DIR, \"metrics\")            # Path to metrics directory\n",
        "\n",
        "# Paths to datasets used for training or evaluation\n",
        "DATASETS = {\n",
        "    \"cornell\": os.path.join(DATA_DIR, \"cornell\"),          # cornell movie-corpus dataset\n",
        "    \"Joey\": os.path.join(DATA_DIR, \"friends\", \"Joey\"),     # Joey dataset (from Friends dataset)\n",
        "    \"Phoebe\": os.path.join(DATA_DIR, \"friends\", \"Phoebe\")  # Phoebe dataset (from Friends dataset)\n",
        "}\n",
        "DATASETS_ID = [\"cornell\", \"Joey\", \"Phoebe\"]                # Dataset identifiers for iteration or selection\n",
        "\n",
        "# Pre-trained conversational model\n",
        "MODEL_NAME = \"microsoft/DialoGPT-small\"             # Name of the HuggingFace pre-trained model to load\n",
        "EXCLUDED_FROM_DECAY = [\"bias\", \"LayerNorm.weight\"]  # Parameters to exclude from weight decay during training\n",
        "DEFAULT_PADDING_SIDE = \"left\"                       # Padding direction for tokenized inputs (GPT models require left padding)\n",
        "\n",
        "# Training and Hyperparameters\n",
        "BLOCK_SIZE = 512                                    # Maximum sequence length for input text\n",
        "TRAIN_BATCH_SIZE = 8                                # Batch size for training\n",
        "EVAL_BATCH_SIZE = 16                                # Batch size for evaluation\n",
        "GRADIENT_ACCUMULATION_STEPS = 2                     # Number of steps to accumulate gradients before updating\n",
        "LEARNING_RATE = 2e-5                                # Initial learning rate for the optimizer\n",
        "WEIGHT_DECAY = 0.01                                 # Weight decay for the optimizer\n",
        "ADAM_EPSILON = 1e-8                                 # Epsilon value for the Adam optimizer\n",
        "MAX_GRAD_NORM = 1.0                                 # Maximum norm for gradient clipping\n",
        "NUM_EPOCHS = 5                                      # Number of training epochs\n",
        "WARMUP_STEPS = 0.01                                 # Number of steps for learning rate warm-up\n",
        "MAX_TARGET_LENGTH = 50                              # Maximum length of the generated response/output sequence\n",
        "SEED = 42                                           # Random seed for reproducibility\n",
        "LOCAL_RANK = -1                                     # Rank of the process for distributed training (-1 for non-distributed)\n",
        "\n",
        "# Type Alias\n",
        "TorchDataset = torch.utils.data.Dataset             # Alias for PyTorch dataset type\n",
        "TorchOptimizer = torch.optim.Optimizer              # Alias for PyTorch optimizer type\n",
        "TorchScheduler = torch.optim.lr_scheduler.LambdaLR  # Alias for a learning rate scheduler (LambdaLR in this case)\n",
        "\n",
        "# Warnings Set-up\n",
        "warnings.filterwarnings(\"ignore\") # Suppress warnings for cleaner output during execution\n",
        "\n",
        "# Emotion Classification Model & Labels\n",
        "EMOTION_MODEL = \"j-hartmann/emotion-english-distilroberta-base\"                        # Pre-trained model for emotion classification.\n",
        "EMOTION_LABELS = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"] # Emotion labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_pWhwWiMcv1"
      },
      "outputs": [],
      "source": [
        "# === Check available device === #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"> Device: {device}\")\n",
        "\n",
        "# Print GPU info.\n",
        "if device.type == \"cuda\":\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\">> WARNING no gpu available! Running via gpu is highly recommended.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define Working environment === #\n",
        "def makedirs(dir_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Create a directory if it doesn't exist.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the directory to be created.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"> Directory '{dir_path}' created.\")\n",
        "    else:\n",
        "        print(f\"> Directory '{dir_path}' already exists.\")\n",
        "\n",
        "\n",
        "# Define environmental directories\n",
        "for path in [OUTPUT_DIR, PLOT_DIR, METRICS_DIR]:\n",
        "    makedirs(dir_path=path)\n",
        "\n",
        "# Define specific directories by datasets id\n",
        "for id in DATASETS_ID:\n",
        "    makedirs(dir_path=os.path.join(OUTPUT_DIR, f\"{id}\"))\n"
      ],
      "metadata": {
        "id": "pDrhbAlqXYtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "hWrgTpvn8eNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset splits\n",
        "def load_dataset(name: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Load the dataset selected by name.\n",
        "\n",
        "    Args:\n",
        "        name: Name of the dataset to load.\n",
        "\n",
        "    Returns:\n",
        "        dataset_dict: A dictionary containing train, valid, and test DataFrames.\n",
        "    \"\"\"\n",
        "    path = DATASETS[name]\n",
        "\n",
        "    dataset_dict = {\n",
        "        \"train\": pd.read_csv(os.path.join(path, f\"{name}_train.csv\")),\n",
        "        \"valid\": pd.read_csv(os.path.join(path, f\"{name}_valid.csv\")),\n",
        "        \"test\": pd.read_csv(os.path.join(path, f\"{name}_test.csv\"))\n",
        "    }\n",
        "    return dataset_dict\n"
      ],
      "metadata": {
        "id": "aZgyz0py8iI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6GpBAykM-Yb"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsY8nMYNxkld"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_jxooWExp1R"
      },
      "outputs": [],
      "source": [
        "# Tokenize and flatten a conversation into a single sequence\n",
        "def tokenize_and_flatten_conversation(dialogue_turns: List[str], tokenizer: PreTrainedTokenizer) -> List[int]:\n",
        "  \"\"\"\n",
        "  Encodes a conversation into a single flattened sequence of tokens.\n",
        "\n",
        "  - Each dialogue turn is encoded and appended with an EOS token.\n",
        "  - The conversation is reversed so the most recent utterance comes first\n",
        "    (beneficial for causal models to focus on the most recent context).\n",
        "\n",
        "  Args:\n",
        "      dialogue_turns: List of dialogue turns in a conversation.\n",
        "      tokenizer: Tokenizer for encoding text.\n",
        "\n",
        "  Returns:\n",
        "      Flattened sequence of token IDs with EOS tokens appended.\n",
        "  \"\"\"\n",
        "  reversed_turns = reversed(dialogue_turns) # Reverse order so latest turns are earlier\n",
        "\n",
        "  tokenized_turns = [\n",
        "      tokenizer.encode(turn, truncation=True, max_length=tokenizer.model_max_length) + [tokenizer.eos_token_id]\n",
        "      for turn in reversed_turns\n",
        "  ]\n",
        "  tokenized_conversation = list(chain.from_iterable(tokenized_turns))\n",
        "  return tokenized_conversation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC1NbjWQxxzO"
      },
      "source": [
        "### Build Conversation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ege6jJHgx6np"
      },
      "outputs": [],
      "source": [
        "# PyTorch-compatible dataset\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, dataframe: pd.DataFrame, tokenizer: PreTrainedTokenizer):\n",
        "        \"\"\"\n",
        "        Initializes a dataset object by preprocessing and tokenizing dialogue rows.\n",
        "\n",
        "        - Adjusts block size to avoid model-specific overflow.\n",
        "        - Iterates through the DataFrame and tokenizes each conversation.\n",
        "        - Filters out overly long sequences to maintain uniformity in batch sizes.\n",
        "\n",
        "        Args:\n",
        "            dataframe: DataFrame of context-response rows.\n",
        "            tokenizer: Tokenizer used to convert text to token IDs.\n",
        "        \"\"\"\n",
        "        # Adjust block size for tokenizer-specific constraints\n",
        "        tokenizer_adjusted_block_size = min(BLOCK_SIZE, tokenizer.model_max_length)\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        for _, row in dataframe.iterrows():\n",
        "            # Tokenize all dialogue turns (row is a Series of context columns + response)\n",
        "            conversation = tokenize_and_flatten_conversation(\n",
        "                dialogue_turns=row.dropna().astype(str).tolist(), tokenizer=tokenizer\n",
        "            )\n",
        "            # Keep only conversations within the allowed length\n",
        "            if len(conversation) <= tokenizer_adjusted_block_size:\n",
        "                self.examples.append(conversation)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # Return a token sequence as a PyTorch tensor\n",
        "        example = self.examples[item]\n",
        "        return torch.tensor(example, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Builds a ConversationDataset instance\n",
        "def build_conversation_dataset(tokenizer: PreTrainedTokenizer, df: pd.DataFrame) -> ConversationDataset:\n",
        "    \"\"\"\n",
        "    Selects the correct DataFrame and builds a dataset object.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: The tokenizer.\n",
        "        df: Selected dataframe.\n",
        "\n",
        "    Returns:\n",
        "        A ConversationDataset instance.\n",
        "    \"\"\"\n",
        "    new_instance = ConversationDataset(dataframe=df, tokenizer=tokenizer)\n",
        "\n",
        "    return new_instance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct4ftcc1yFXk"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ciB0lbVyIuw"
      },
      "outputs": [],
      "source": [
        "# Ensures reproducibility\n",
        "def set_seed() -> None:\n",
        "    \"\"\"Ensure deterministic behavior across runs.\"\"\"\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "# Pads a batch of sequences to the maximum length in the batch\n",
        "def collate_function(examples: List[torch.Tensor], tokenizer: PreTrainedTokenizer) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Collates a batch of variable-length token sequences into a uniform batch with padding.\n",
        "\n",
        "    Args:\n",
        "        examples: List of token tensors.\n",
        "        tokenizer: Tokenizer with pad token information.\n",
        "\n",
        "    Returns:\n",
        "        A single padded batch tensor of shape (batch_size, max_seq_len).\n",
        "    \"\"\"\n",
        "    if tokenizer.pad_token is not None:\n",
        "        return pad_sequence(\n",
        "            examples, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "        )\n",
        "    else:\n",
        "        return pad_sequence(examples, batch_first=True)\n",
        "\n",
        "\n",
        "# Builds a PyTorch DataLoader for training or evaluation\n",
        "def get_dataloader(dataset: TorchDataset, tokenizer: PreTrainedTokenizer, is_training: bool) -> DataLoader:\n",
        "    \"\"\"\n",
        "    Creates a DataLoader object.\n",
        "\n",
        "    Args:\n",
        "        dataset: Preprocessed dataset object.\n",
        "        tokenizer: Tokenizer used for collate function.\n",
        "        is_training: Flag to set batch size and sampler mode.\n",
        "\n",
        "    Returns:\n",
        "        A DataLoader instance ready for training or evaluation.\n",
        "    \"\"\"\n",
        "    # Determine appropriate batch size\n",
        "    batch_size = TRAIN_BATCH_SIZE if is_training else EVAL_BATCH_SIZE\n",
        "    shuffle_tag = True if is_training else False\n",
        "\n",
        "    # Return DataLoader\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=lambda examples: collate_function(\n",
        "            examples=examples, tokenizer=tokenizer\n",
        "        ),\n",
        "        shuffle=shuffle_tag,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fm2JmIkyOLi"
      },
      "source": [
        "### Optimizer and Scheduler Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAWHIGp6J1NP"
      },
      "outputs": [],
      "source": [
        "# Prepares optimizer and scheduler\n",
        "def configure_optimizer_scheduler(model: PreTrainedModel, total_training_steps: int) -> Tuple[TorchOptimizer, TorchScheduler]:\n",
        "    \"\"\"\n",
        "    Sets up AdamW optimizer and linear learning rate scheduler.\n",
        "\n",
        "    - Applies weight decay to all layers except specified exclusions.\n",
        "    - Uses linear decay with warmup steps to stabilize early training.\n",
        "\n",
        "    Args:\n",
        "        model: Transformer model with named parameters.\n",
        "        total_training_steps: Total steps across all epochs.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (optimizer, scheduler).\n",
        "    \"\"\"\n",
        "    optimizer_groups = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in model.named_parameters()\n",
        "                if not any(exclusion in n for exclusion in EXCLUDED_FROM_DECAY)\n",
        "            ],\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in model.named_parameters()\n",
        "                if any(exclusion in n for exclusion in EXCLUDED_FROM_DECAY)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = AdamW(\n",
        "        params=optimizer_groups, lr=LEARNING_RATE, eps=ADAM_EPSILON,\n",
        "    )\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=int(total_training_steps * WARMUP_STEPS),\n",
        "        num_training_steps=total_training_steps,\n",
        "    )\n",
        "    return optimizer, scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT0Ij7wayaNz"
      },
      "source": [
        "### Model & Tokenizer I/O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doD4CB3dygHZ"
      },
      "outputs": [],
      "source": [
        "# Loads a pre-trained model and tokenizer\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained transformer model and tokenizer from specified paths.\n",
        "\n",
        "    Args:\n",
        "        model_path: Directory path to the model.\n",
        "        tokenizer_path: Directory path to the tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, tokenizer).\n",
        "    \"\"\"\n",
        "    # configuration\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        gradient_checkpointing=True\n",
        "    )\n",
        "\n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path=tokenizer_path,\n",
        "        config=config,\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = DEFAULT_PADDING_SIDE\n",
        "\n",
        "    # Model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        config=config,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Saves model and tokenizer after training\n",
        "def save_model_and_tokenizer(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, data_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Save the final model and tokenizer to path.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned transformer model.\n",
        "        tokenizer: Associated tokenizer.\n",
        "        data_id: identify the data to save to correct location.\n",
        "    \"\"\"\n",
        "    save_path = os.path.join(OUTPUT_DIR, data_id)\n",
        "    print(f\"\\n> Saving model to: {save_path}\")\n",
        "\n",
        "    model.config.save_pretrained(save_path)\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "UwicTsH7GYEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training metrics\n",
        "def training_scores_to_csv(data_id: str, **metrics: list) -> None:\n",
        "    \"\"\"\n",
        "    Save metrics calculated during training to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        data_id: Identify the dataset to save to correct location.\n",
        "        **metrics: Metric lists (e.g., Train_Loss, Val_Loss).\n",
        "    \"\"\"\n",
        "    metrics_df = pd.DataFrame({\"Epoch\": list(range(1, NUM_EPOCHS + 1))})\n",
        "\n",
        "    # Add each metric to the DataFrame\n",
        "    for name, values in metrics.items():\n",
        "        metrics_df[name.replace(\"_\", \" \")] = [round(v, 4) for v in values]\n",
        "\n",
        "    csv_path = os.path.join(METRICS_DIR, f\"{data_id}_training_metrics.csv\")\n",
        "    metrics_df.to_csv(path_or_buf=csv_path, index=False)\n",
        "\n",
        "    print(f\"\\n> Metrics saved to: {csv_path}\")\n",
        "\n",
        "\n",
        "# Compute BLEU, ROUGE-L, BERTScore (F1)\n",
        "def compute_nlp_metrics(references: List[str], predictions: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes NLP metrics for model evaluation.\n",
        "\n",
        "    Args:\n",
        "        references: List of ground truth responses.\n",
        "        predictions: List of generated responses.\n",
        "\n",
        "    Returns:\n",
        "        A Dictionary with BLEU, ROUGE-L and BERTScore(F1) scores.\n",
        "    \"\"\"\n",
        "    references = [str(ref) for ref in references]\n",
        "\n",
        "    # BLEU\n",
        "    bleu_scores = [sentence_bleu([ref.split()], pred.split()) for ref, pred in zip(references, predictions)]\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "    rouge_l_scores = [rouge_scorer_instance.score(ref, pred)[\"rougeL\"].fmeasure for ref, pred in zip(references, predictions)]\n",
        "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "\n",
        "    # BERTScore (F1)\n",
        "    _, _, F1 = score(predictions, references, lang=\"en\", rescale_with_baseline=True)\n",
        "    avg_bert_f1 = F1.mean().item()\n",
        "\n",
        "    return {\n",
        "        \"BLEU\": avg_bleu,\n",
        "        \"ROUGE-L\": avg_rouge_l,\n",
        "        \"BERTScore-F1\": avg_bert_f1\n",
        "    }\n",
        "\n",
        "\n",
        "# Store evaluation metrics\n",
        "def evaluation_metrics_to_csv(data_id: str, bleu: float, rouge: float, bertscore: float) -> None:\n",
        "    \"\"\"\n",
        "    Save NLP metrics (BLEU, ROUGE-L and BERTScore(F1)) to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        predictions: Generated responses from the model.\n",
        "        references: Ground truth responses.\n",
        "        save_path: Path to the output CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        \"BLEU\": [bleu],\n",
        "        \"ROGUE-L\": [rouge],\n",
        "        \"BERTScore-F1\": [bertscore],\n",
        "    })\n",
        "    csv_path = os.path.join(METRICS_DIR, f\"{data_id}_predicts_metrics.csv\")\n",
        "    df.to_csv(path_or_buf=csv_path, index=False)\n",
        "\n",
        "    print(f\"\\n> Metrics saved to: {csv_path}\")\n"
      ],
      "metadata": {
        "id": "4zTFV9SNGZ21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQG6XUbVh9a3"
      },
      "source": [
        "### Plot Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BXQgnkCh_Fw"
      },
      "outputs": [],
      "source": [
        "# Plot average training/validation loss per epoch\n",
        "def plot_loss(csv_path: str, save_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Plot Train and Val loss across epochs.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the CSV file containing metrics.\n",
        "        save_path: Path to save the resulting plot image.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Train loss\n",
        "    plt.plot(\n",
        "        df[\"Epoch\"], df[\"Train Loss\"],\n",
        "        marker=\"o\", linestyle=\"-\",\n",
        "        color=\"royalblue\", linewidth=2,\n",
        "        label=\"Avg. Training Loss\"\n",
        "    )\n",
        "\n",
        "    # Val loss\n",
        "    plt.plot(\n",
        "        df[\"Epoch\"], df[\"Val Loss\"],\n",
        "        marker=\"s\", linestyle=\"--\",\n",
        "        color=\"orange\", linewidth=2,\n",
        "        label=\"Avg. Validation Loss\"\n",
        "    )\n",
        "\n",
        "    # Labels, title, and grid\n",
        "    plt.title(\"Training and Validation Loss per Epoch\", fontsize=16)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.ylabel(\"Loss\", fontsize=14)\n",
        "    plt.grid(True, linestyle='-', alpha=0.6)\n",
        "    plt.xticks(df[\"Epoch\"])\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(save_path)  # Save the plot\n",
        "    plt.show() # Show the plot\n",
        "\n",
        "\n",
        "# Plot average perplexity per epoch\n",
        "def plot_perplexity(csv_path: str, save_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Plot Perplexity across epochs.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the CSV file containing metrics.\n",
        "        save_path: Path to save the resulting plot image.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Perplexity\n",
        "    plt.plot(\n",
        "        df[\"Epoch\"], df[\"Perplexity\"],\n",
        "        marker=\"o\", linestyle=\"-\",\n",
        "        color=\"forestgreen\", linewidth=2,\n",
        "        label=\"Avg. Perpexity\"\n",
        "    )\n",
        "\n",
        "    # Labels, title, and grid\n",
        "    plt.title(\"Perplexity scores per Epoch\", fontsize=16)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.ylabel(\"Perplexity\", fontsize=14)\n",
        "    plt.grid(True, linestyle='-', alpha=0.6)\n",
        "    plt.xticks(df[\"Epoch\"])\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(save_path)  # Save the plot\n",
        "    plt.show() # Show the plot\n",
        "\n",
        "\n",
        "# Plot Emotion radar\n",
        "def plot_emotion_radar(ref_avg: np.ndarray, pred_avg: np.ndarray, labels: list, id: str, save_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Plot reference and predicted average emotion scores as a radar chart.\n",
        "\n",
        "    Args:\n",
        "        ref_avg: Array of average emotion scores from the reference texts.\n",
        "        pred_avg: Array of average emotion scores from the predicted texts.\n",
        "        labels: List of emotion label names.\n",
        "        id: Identify the dataset.\n",
        "        save_path: File path to save the generated radar plot.\n",
        "    \"\"\"\n",
        "    # Filter out \"neutral\" from labels and scores\n",
        "    filtered = [(label, r, p) for label, r, p in zip(labels, ref_avg, pred_avg) if label != \"neutral\"]\n",
        "    filtered_labels, ref_avg, pred_avg = zip(*filtered)\n",
        "    ref_avg = np.array(ref_avg)\n",
        "    pred_avg = np.array(pred_avg)\n",
        "\n",
        "    # Compute angles for radar chart (one per emotion label)\n",
        "    angles = np.linspace(0, 2 * np.pi, len(filtered_labels), endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    ref_avg = np.append(ref_avg, ref_avg[0])\n",
        "    pred_avg = np.append(pred_avg, pred_avg[0])\n",
        "\n",
        "    # Define radar plot\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "    # Plot reference line\n",
        "    ax.plot(angles, ref_avg, color=\"blue\", linewidth=2, label=\"Reference\")\n",
        "    ax.fill(angles, ref_avg, color=\"blue\", alpha=0.25)\n",
        "    # Plot prediction line\n",
        "    ax.plot(angles, pred_avg, color=\"red\", linewidth=2, label=\"Prediction\")\n",
        "    ax.fill(angles, pred_avg, color=\"red\", alpha=0.25)\n",
        "\n",
        "    # Label\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), filtered_labels)\n",
        "    ax.set_title(f\"{id} - Emotion Spectrum\", size=16, pad=30)\n",
        "    ax.legend(loc=\"best\", bbox_to_anchor=(1.1, 1.1))\n",
        "    # Save the plot\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xq-pMGERr4h"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVTBkNzNHTBu"
      },
      "outputs": [],
      "source": [
        "# === Training and Validation Steps === #\n",
        "\n",
        "# Training Step\n",
        "def training_step(model: PreTrainedModel, optimizer: TorchOptimizer, scheduler: TorchScheduler, dataloader: DataLoader, scaler: GradScaler) -> float:\n",
        "    \"\"\"\n",
        "    Executes a single epoch of training.\n",
        "\n",
        "    Args:\n",
        "        model: The model being trained.\n",
        "        optimizer: The optimizer used for training.\n",
        "        scheduler: Learning rate scheduler.\n",
        "        dataloader: The DataLoader for training data.\n",
        "        scaler: GradScaler for mixed precision training.\n",
        "\n",
        "    Returns:\n",
        "        Average loss for the epoch.\n",
        "    \"\"\"\n",
        "    model.train() # Set model in training mode\n",
        "    loss_counter = 0.0 # Initialize counter\n",
        "\n",
        "    # Iterate through training batches with a progress bar\n",
        "    epoch_iterator = tqdm(dataloader, desc=\"Training Progress\")\n",
        "\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        # In causal LM, inputs = labels\n",
        "        inputs = batch.to(device) # Move input to the designated device\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(inputs, labels=inputs) # Forward pass with loss computation\n",
        "            loss = outputs.loss # Extract scalar loss value\n",
        "        scaler.scale(loss).backward() # Backpropagation\n",
        "        loss_counter += loss.item() # Accumulate loss for average computation\n",
        "\n",
        "        # Perform optimization step every gradient_accumulation_steps\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "\n",
        "            current_lr = scheduler.get_last_lr()[0] # Get current learning rate\n",
        "            epoch_iterator.set_postfix({\"loss\": loss.item(), \"lr\": current_lr})\n",
        "\n",
        "            # Gradient clipping to stabilize training\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "            # Update model parameters\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "             # Update learning rate\n",
        "            scheduler.step()\n",
        "            # Reset gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "    # Average loss\n",
        "    avg_train_loss = loss_counter / len(dataloader)\n",
        "\n",
        "    return avg_train_loss\n",
        "\n",
        "\n",
        "# Validation Step\n",
        "def validation_step(model: PreTrainedModel, dataloader: DataLoader) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Executes a single validation step.\n",
        "\n",
        "    Args:\n",
        "        model: The model being evaluated.\n",
        "        dataloader: The DataLoader for validation data.\n",
        "\n",
        "    Returns:\n",
        "        Loss for the validation step.\n",
        "        Perplexity for the validation step.\n",
        "    \"\"\"\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    loss_counter = 0.0\n",
        "    perplexity_counter = 0.0\n",
        "\n",
        "    # Iterate through validation batches with a progress bar\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation Progress\"):\n",
        "            # In causal LM, inputs = labels\n",
        "            inputs = batch.to(device) # Move input to the designated device\n",
        "            outputs = model(input_ids=inputs, labels=inputs) # Forward pass with loss computation\n",
        "            loss_counter += outputs.loss.item()\n",
        "            perplexity_counter += torch.exp(outputs.loss).item()\n",
        "\n",
        "    # Average validation loss\n",
        "    avg_val_loss = loss_counter / len(dataloader)\n",
        "    # Average perplexity\n",
        "    avg_perplexity = perplexity_counter / len(dataloader)\n",
        "\n",
        "    return avg_val_loss, avg_perplexity\n",
        "\n",
        "\n",
        "# === Complete Training Process === #\n",
        "def perform_training_process(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, train_set: ConversationDataset, valid_set: ConversationDataset, id: str) -> None:\n",
        "    \"\"\"\n",
        "    Executes full training loop across multiple epochs.\n",
        "\n",
        "    Args:\n",
        "        model: The model to be trained.\n",
        "        tokenizer: The employed tokenizer.\n",
        "        train_dataset: The dataset for training.\n",
        "        valid_dataset: The dataset for validation.\n",
        "        id: Identify the dataset to save to correct location.\n",
        "\n",
        "    Returns:\n",
        "        Average Training/Validation Loss and Average Perplexity.\n",
        "    \"\"\"\n",
        "    # Train DataLoader\n",
        "    train_dataloader = get_dataloader(\n",
        "        dataset=train_set,\n",
        "        tokenizer=tokenizer,\n",
        "        is_training=True,\n",
        "    )\n",
        "    # Validation DataLoader\n",
        "    valid_dataloader = get_dataloader(\n",
        "        dataset=valid_set,\n",
        "        tokenizer=tokenizer,\n",
        "        is_training=False,\n",
        "    )\n",
        "\n",
        "    # Calculate total number of optimization steps\n",
        "    total_steps = len(train_dataloader) * NUM_EPOCHS\n",
        "    # Resize model embeddings in case tokenizer has been extended\n",
        "    model.resize_token_embeddings(new_num_tokens=len(tokenizer))\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer, scheduler = configure_optimizer_scheduler(\n",
        "        model=model,\n",
        "        total_training_steps=total_steps,\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "    set_seed() # Set seed for reproducibility\n",
        "\n",
        "    print(\"\\n>> START TRAINING PROCESS\")\n",
        "    print(f\" -- Training Examples: {len(train_dataloader)}\")\n",
        "    print(f\" -- Validation Examples: {len(valid_dataloader)}\")\n",
        "    print(f\" -- Number of Epochs: {NUM_EPOCHS}\\n\")\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses = [] # Train Loss per epoch storing (used by plot)\n",
        "    valid_losses = [] # Valid Loss per epoch storing (used by plot)\n",
        "    perplexities = [] # Perplexity per epoch storing\n",
        "\n",
        "    # Iterator\n",
        "    train_iterator = trange(NUM_EPOCHS, desc=\"Epoch Iteration\")\n",
        "\n",
        "    # Main training loop over all epochs\n",
        "    for epoch in train_iterator:\n",
        "        print(f\"\\n> Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "        # Training\n",
        "        train_loss = training_step(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            dataloader=train_dataloader,\n",
        "            scaler=scaler,\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, perplexity = validation_step(\n",
        "            model=model, dataloader=valid_dataloader\n",
        "        )\n",
        "        valid_losses.append(val_loss)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "        print(f\"\\n  - Training Loss: {train_loss:.4f}\")\n",
        "        print(f\"  - Validation Loss: {val_loss:.4f}\")\n",
        "        print(f\"  - Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "    # Avg. Metrics values through all epochs\n",
        "    avg_training_loss = sum(train_losses) / len(train_losses)\n",
        "    avg_validation_loss = sum(valid_losses) / len(valid_losses)\n",
        "    avg_perplexity = sum(perplexities) / len(perplexities)\n",
        "\n",
        "    print(\n",
        "        f\"\\n> Avg. Train Loss: {avg_training_loss:.4f}, \"\n",
        "        f\"Avg. Val Loss: {avg_validation_loss:.4f}, \"\n",
        "        f\"Avg. Perplexity: {avg_perplexity:.4f}.\"\n",
        "    )\n",
        "    # Save to csv\n",
        "    training_scores_to_csv(\n",
        "        data_id=id, Train_Loss=train_losses, Val_Loss=valid_losses, Perplexity=perplexities\n",
        "    )\n",
        "    # Save trained model and tokenizer\n",
        "    save_model_and_tokenizer(model=model, tokenizer=tokenizer, data_id=id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train all dataset"
      ],
      "metadata": {
        "id": "eCFvgYqAB7na"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5UV3eD1U0VU"
      },
      "outputs": [],
      "source": [
        "# Execute the training process on all dataset\n",
        "def execute_training_on_all_dataset() -> None:\n",
        "    \"\"\"\n",
        "    Execute the training process on all datasets.\n",
        "\n",
        "    Args:\n",
        "        train_df: Training DataFrame.\n",
        "        val_df: Validation DataFrame.\n",
        "    \"\"\"\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model, tokenizer = load_model_and_tokenizer(\n",
        "        tokenizer_path=MODEL_NAME,\n",
        "        model_path=MODEL_NAME,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Train model on every dataset\n",
        "    for id in DATASETS_ID:\n",
        "        print(f\"\\n>> Training on {id} data...\")\n",
        "\n",
        "        # Load dataset splits\n",
        "        dataset_splits = load_dataset(name=id)\n",
        "        train_df = dataset_splits[\"train\"]\n",
        "        val_df = dataset_splits[\"valid\"]\n",
        "\n",
        "        # Training Dataset\n",
        "        training_dataset = build_conversation_dataset(\n",
        "            tokenizer=tokenizer,\n",
        "            df=train_df,\n",
        "        )\n",
        "        # Validation Dataset\n",
        "        validation_dataset = build_conversation_dataset(\n",
        "            tokenizer=tokenizer,\n",
        "            df=val_df,\n",
        "        )\n",
        "        # Training Process\n",
        "        perform_training_process(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            train_set=training_dataset,\n",
        "            valid_set=validation_dataset,\n",
        "            id=id,\n",
        "        )\n",
        "\n",
        "        # Plot loss and Perplexity\n",
        "        csv_path = os.path.join(METRICS_DIR, f\"{id}_training_metrics.csv\")\n",
        "        plot_loss_path = os.path.join(PLOT_DIR, f\"{id}_train_val_loss_plot.png\")\n",
        "        plot_perplexity_path = os.path.join(PLOT_DIR, f\"{id}_perplexity_plot.png\")\n",
        "\n",
        "        plot_loss(csv_path=csv_path, save_path=plot_loss_path)\n",
        "        plot_perplexity(csv_path=csv_path, save_path=plot_perplexity_path)\n",
        "\n",
        "        print(f\"-\"*30) # Separation line for readability\n",
        "        print() # Empty line for readability\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Traning Execution on all data === #\n",
        "execute_training_on_all_dataset()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4KpnFmvNHEmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose which dataset to train"
      ],
      "metadata": {
        "id": "mSvhB5QCB_fF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSRoxyn5JKmk"
      },
      "outputs": [],
      "source": [
        "# Execute the training process on a single selected datset\n",
        "def execute_training_on_single_dataset(dataset_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Execute the training process on a single dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_id: ID of the dataset to be used.\n",
        "    \"\"\"\n",
        "    if dataset_id not in DATASETS_ID:\n",
        "        raise ValueError(f\"Unknown dataset_id: {dataset_id}\")\n",
        "\n",
        "    # Load dataset splits\n",
        "    dataset_splits = load_dataset(name=dataset_id)\n",
        "    train_df = dataset_splits[\"train\"]\n",
        "    valid_df = dataset_splits[\"valid\"]\n",
        "\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model, tokenizer = load_model_and_tokenizer(\n",
        "        tokenizer_path=MODEL_NAME,\n",
        "        model_path=MODEL_NAME,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Build Training Dataset\n",
        "    training_dataset = build_conversation_dataset(\n",
        "        tokenizer=tokenizer, df=train_df\n",
        "    )\n",
        "    # Build Validation Dataset\n",
        "    validation_dataset = build_conversation_dataset(\n",
        "        tokenizer=tokenizer, df=valid_df\n",
        "    )\n",
        "\n",
        "    # Execute the training process\n",
        "    perform_training_process(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_set=training_dataset,\n",
        "        valid_set=validation_dataset,\n",
        "        id=dataset_id,\n",
        "    )\n",
        "    # Plot loss and Perplexity\n",
        "    csv_path = os.path.join(METRICS_DIR, f\"{dataset_id}_training_metrics.csv\")\n",
        "    plot_loss_path = os.path.join(PLOT_DIR, f\"{dataset_id}_train_val_loss_plot.png\")\n",
        "    plot_perplexity_path = os.path.join(PLOT_DIR, f\"{dataset_id}_perplexity_plot.png\")\n",
        "    plot_loss(csv_path=csv_path, save_path=plot_loss_path)\n",
        "    plot_perplexity(csv_path=csv_path, save_path=plot_perplexity_path)\n",
        "\n",
        "\n",
        "# Choose the datset to training on\n",
        "def prompt_and_execute_training() -> None:\n",
        "    \"\"\"\n",
        "    Prompt the user to enter a valid dataset ID and execute training on it.\n",
        "    Allows exiting the loop with 'exit', 'quit', or 'q'.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        print(\"\\n> Available datasets:\", \", \".join(DATASETS_ID))\n",
        "        user_input = input(\"  Enter the dataset ID to train on (or type 'q' to exit): \").strip()\n",
        "\n",
        "        if user_input.lower() in {\"q\"}:\n",
        "            print(\"\\n> Process end by user.\")\n",
        "            return\n",
        "\n",
        "        if user_input in DATASETS_ID:\n",
        "            print(f\"\\n> Starting training on '{user_input}' dataset...\")\n",
        "            execute_training_on_single_dataset(user_input)\n",
        "            return  # Exit after successful training\n",
        "\n",
        "        print(f\"\\n> Invalid dataset ID: '{user_input}'. Please choose from: {', '.join(DATASETS_ID)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Choose one and Train === #\n",
        "prompt_and_execute_training()\n"
      ],
      "metadata": {
        "id": "6jZEk3PHRDnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTtZdEMERurH"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss and Perplexity"
      ],
      "metadata": {
        "id": "UDHA5KP0Avil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW_gE5WImaWw"
      },
      "outputs": [],
      "source": [
        "# Perplexity and Loss\n",
        "def perform_model_evaluation(model: PreTrainedModel, data: DataLoader) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes evaluation loss and perplexity.\n",
        "\n",
        "    Args:\n",
        "        trained_model: The model to evaluate.\n",
        "        data: DataLoader for evaluation data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing evaluation loss and perplexity.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    model.to(device)\n",
        "\n",
        "    loss_values = []\n",
        "    perplexities = []\n",
        "\n",
        "    for batch in tqdm(data, desc=\"Evaluation on Test set\"):\n",
        "\n",
        "        inputs = batch.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(inputs, labels=inputs)\n",
        "            loss = outputs.loss\n",
        "            loss_values.append(loss.item())\n",
        "            perplexities.append(torch.exp(loss).item())\n",
        "\n",
        "    avg_test_loss = sum(loss_values) / len(loss_values)\n",
        "    avg_test_perplexity = sum(perplexities) / len(perplexities)\n",
        "\n",
        "    return avg_test_loss, avg_test_perplexity\n",
        "\n",
        "\n",
        "# Perform evaluation on all dataset\n",
        "def execute_evaluation_on_all_dataset() -> None:\n",
        "    \"\"\"\n",
        "    Execute the evaluation process on all datasets.\n",
        "    \"\"\"\n",
        "    for id in DATASETS_ID:\n",
        "        print(f\"\\n>> Evaluation on {id} data...\")\n",
        "\n",
        "        # Load dataset splits\n",
        "        dataset_splits = load_dataset(name=id)\n",
        "        test_df = dataset_splits[\"test\"]\n",
        "\n",
        "        # Path to trained Model & Tokenizer\n",
        "        model_tokenizer_path = os.path.join(OUTPUT_DIR, id)\n",
        "\n",
        "        # Load Model & Tokenizer\n",
        "        model, tokenizer = load_model_and_tokenizer(\n",
        "            tokenizer_path=model_tokenizer_path,\n",
        "            model_path=model_tokenizer_path,\n",
        "        )\n",
        "\n",
        "        # Build Test Dataset\n",
        "        test_dataset = build_conversation_dataset(\n",
        "            tokenizer=tokenizer,\n",
        "            df=test_df,\n",
        "        )\n",
        "\n",
        "        evaluation_loader = get_dataloader(\n",
        "            dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            is_training=False,\n",
        "        )\n",
        "\n",
        "        # Model evaluation on Test set\n",
        "        avg_loss, avg_perplexity = perform_model_evaluation(model=model, data=evaluation_loader)\n",
        "\n",
        "        print(f\"\\n> Loss: {avg_loss:.4f} - Perplexity: {avg_perplexity:.4f}\\n\")\n",
        "        # Save eval. metrics to csv\n",
        "        df = pd.DataFrame({\"Evaluation Loss\": [avg_loss], \"Evaluation Perplexity\": [avg_perplexity]})\n",
        "        df.to_csv(os.path.join(METRICS_DIR, f\"{id}_evaluation_metrics.csv\"), index=False)\n",
        "\n",
        "        print(f\"-*-\"*30) # Separation line for readability\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluation Process === #\n",
        "execute_evaluation_on_all_dataset()\n"
      ],
      "metadata": {
        "id": "xiKl5jWcRs0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmaX-Qwbi3Re"
      },
      "source": [
        "### Emotion classification and Scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate model predictions\n",
        "def generate_predictions(df: pd.DataFrame, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, context_column: Optional[int] = None) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Generate model predictions from input DataFrame using provided model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        df: Input data containing context text.\n",
        "        model: Pre-trained model for text generation.\n",
        "        tokenizer: Tokenizer associated with the model.\n",
        "        context_column: Column index for context text.\n",
        "\n",
        "    Returns:\n",
        "        List of generated prediction strings and reference.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_size = 16\n",
        "    max_new_tokens = 64\n",
        "\n",
        "    predictions = []  # model generated responses\n",
        "    full_contexts = []\n",
        "    references = df[\"response\"].tolist()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        if context_column is not None:\n",
        "            # Use only one specific context turn\n",
        "            context_value = str(row.get(f\"context_{context_column}\", \"\"))\n",
        "            context_input = context_value.strip()\n",
        "        else:\n",
        "            # Use full multi-turn context\n",
        "            context_turns = row[[\"context_0\", \"context_1\", \"context_2\", \"context_3\", \"context_4\"]].dropna().astype(str).tolist()\n",
        "            token_ids = tokenize_and_flatten_conversation(dialogue_turns=context_turns, tokenizer=tokenizer)\n",
        "            context_input = tokenizer.decode(token_ids=token_ids, skip_special_tokens=True)\n",
        "\n",
        "        full_contexts.append(context_input)\n",
        "\n",
        "    # Batch prediction loop\n",
        "    for i in tqdm(range(0, len(full_contexts), batch_size), desc=\"Generating Predictions\"):\n",
        "        batch_contexts = full_contexts[i:i + batch_size]\n",
        "        encodings = tokenizer(batch_contexts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = encodings[\"input_ids\"].to(device)\n",
        "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                top_k=50,\n",
        "                top_p=0.7,\n",
        "                do_sample=True,\n",
        "                temperature=0.8,\n",
        "                no_repeat_ngram_size=3,\n",
        "            )\n",
        "\n",
        "        # Decode outputs\n",
        "        for output in outputs:\n",
        "            predictions.append(tokenizer.decode(output, skip_special_tokens=True))\n",
        "\n",
        "    return predictions, references\n",
        "\n",
        "\n",
        "# Load classifier and get emotion scores\n",
        "def get_emotion_scores(input: list[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute emotion scores for a list of texts using a pre-trained classifier.\n",
        "\n",
        "    Args:\n",
        "        input: List of input text strings.\n",
        "\n",
        "    Returns:\n",
        "        Array of shape (len(input), len(EMOTION_LABELS)), where each row contains\n",
        "        the emotion score distribution for a corresponding text.\n",
        "    \"\"\"\n",
        "    # Initialize\n",
        "    all_scores = np.zeros((len(input), len(EMOTION_LABELS)))\n",
        "\n",
        "    # Emotion classification pipeline\n",
        "    emotion_classifier = pipeline(\n",
        "        task=\"text-classification\",\n",
        "        model=EMOTION_MODEL,\n",
        "        batch_size=8,\n",
        "        device=device,\n",
        "        return_all_scores=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    for i, text in enumerate(input):\n",
        "        result = emotion_classifier(str(text))[0]  # Get emotion scores\n",
        "        scores = {item[\"label\"].lower(): item[\"score\"] for item in result}\n",
        "\n",
        "        # Map emotion scores to the correct index\n",
        "        for j, label in enumerate(EMOTION_LABELS):\n",
        "            all_scores[i, j] = scores.get(label, 0)\n",
        "\n",
        "    return all_scores\n"
      ],
      "metadata": {
        "id": "F77hDWpvT5Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions based on dataset\n",
        "def evaluate_on_predictions_all_dataset() -> None:\n",
        "    \"\"\"\n",
        "    Generate predictions for all datasets and store them.\n",
        "    \"\"\"\n",
        "    for id in DATASETS_ID:\n",
        "        print(f\"\\n>> Predicts on {id} data...\")\n",
        "\n",
        "        # Load dataset splits\n",
        "        dataset_splits = load_dataset(name=id)\n",
        "        test_df = dataset_splits[\"test\"]\n",
        "\n",
        "        # Path to trained Model & Tokenizer\n",
        "        model_tokenizer_path = os.path.join(OUTPUT_DIR, id)\n",
        "\n",
        "        # Load Model & Tokenizer\n",
        "        model, tokenizer = load_model_and_tokenizer(\n",
        "            tokenizer_path=model_tokenizer_path,\n",
        "            model_path=model_tokenizer_path,\n",
        "        )\n",
        "        tokenizer.padding_side = DEFAULT_PADDING_SIDE\n",
        "\n",
        "        # Generate prediction\n",
        "        predictions, references = generate_predictions(df=test_df, model=model, tokenizer=tokenizer, context_column=0)\n",
        "        # print(f\"\\n> PRED: {predictions}\")\n",
        "        # print(f\"> REF: {references}\\n\")\n",
        "\n",
        "        # Compute metrics\n",
        "        print(f\"\\n> Compute metrics: \")\n",
        "        score = compute_nlp_metrics(predictions=predictions, references=references)\n",
        "        evaluation_metrics_to_csv(data_id=id, bleu=score[\"BLEU\"], rouge=score[\"ROUGE-L\"], bertscore=score[\"BERTScore-F1\"])\n",
        "        print(f\" -- BLEU: {score['BLEU']:.4f} - ROUGE-L: {score['ROUGE-L']:.4f} - BERTScore (F1): {score['BERTScore-F1']:.4f}\\n\")\n",
        "\n",
        "        # Emotion Classification\n",
        "        ref_emo_scores = get_emotion_scores(input=references)\n",
        "        pred_emo_scores = get_emotion_scores(input=predictions)\n",
        "        avg_ref_scores = np.mean(ref_emo_scores, axis=0)\n",
        "        avg_pred_scores = np.mean(pred_emo_scores, axis=0)\n",
        "\n",
        "        # print(f\"\\n> Reference Emotion Scores: {avg_ref_scores}\")\n",
        "        # print(f\"\\n> Predicted Emotion Scores: {avg_pred_scores}\\n\")\n",
        "\n",
        "        # Plot results\n",
        "        plot_emotion_radar(\n",
        "            ref_avg=avg_ref_scores,\n",
        "            pred_avg=avg_pred_scores,\n",
        "            labels=EMOTION_LABELS,\n",
        "            save_path=os.path.join(PLOT_DIR, f\"{id}_emotion_radar_plot.png\"),\n",
        "            id=id,\n",
        "        )\n",
        "        print(f\"-*-\"*30) # Separation line for readability\n"
      ],
      "metadata": {
        "id": "U-pCTLqaT5Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Predictions for all datasets === #\n",
        "evaluate_on_predictions_all_dataset()\n"
      ],
      "metadata": {
        "id": "XXy9v4OGTEuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bdT7WGeBykw"
      },
      "source": [
        "## Chat with Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2xGWNHBHu_U"
      },
      "outputs": [],
      "source": [
        "# Generate the response\n",
        "def generate_response(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, messages: List[torch.Tensor], max_context: int = 5) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Generates a chatbot response based on the message history.\n",
        "\n",
        "    Args:\n",
        "        model: Pre-trained language model.\n",
        "        tokenizer: Tokenizer used for encoding input text.\n",
        "        messages: List of tokenized conversation history.\n",
        "        max_context: Number of previous exchanges to consider for context. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "        Generated chatbot response as tokenized text.\n",
        "    \"\"\"\n",
        "    # Concatenate the last `max_context` messages to form input\n",
        "    bot_input_ids = torch.cat([m.to(device) for m in messages[-max_context:]], dim=-1)\n",
        "\n",
        "    # Ensure the model has a pad token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # attention mask for proper decoding\n",
        "    attention_mask = (bot_input_ids != tokenizer.pad_token_id).long()\n",
        "\n",
        "    # Generate response\n",
        "    output_ids = model.generate(\n",
        "        input_ids=bot_input_ids,              # Message in input\n",
        "        attention_mask=attention_mask,        # Attention mask to focus on relevant parts\n",
        "        max_length=100,                       # Maximum response length\n",
        "        pad_token_id=tokenizer.eos_token_id,  # Ensure padding works correctly\n",
        "        top_p=0.7,                            # Nucleus sampling for diverse responses\n",
        "        top_k=50,                             # Limits the sampling pool\n",
        "        do_sample=True,                       # Enables non-deterministic sampling\n",
        "        temperature=0.8,                      # Controls randomness (higher = more random)\n",
        "        no_repeat_ngram_size=3,               # Helps to avoids repeating phrases\n",
        "    )\n",
        "    # Strip input from output to isolate the response (excluding input prompt)\n",
        "    response_ids = output_ids[:, bot_input_ids.shape[-1]:]\n",
        "\n",
        "    return response_ids\n",
        "\n",
        "\n",
        "# Chat with bot\n",
        "def chatbot_loop(model_path: str, tokenizer_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Starts an interactive chatbot session using a pre-trained language model.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the pre-trained model. Default is 'output_dir'.\n",
        "        tokenizer_path: Path to the tokenizer. Default is 'tokenizer_name'.\n",
        "    \"\"\"\n",
        "    messages = [] # Store conversation history\n",
        "\n",
        "    # Load chatbot model and tokenizer\n",
        "    model, tokenizer = load_model_and_tokenizer(\n",
        "        model_path=model_path, tokenizer_path=tokenizer_path\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Chatbot initialized! Type 'bye', 'quit', or 'exit' to end the chat]\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input and normalize it\n",
        "        user_input = input(\"\\n>> User: \").strip().lower()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input in {\"bye\", \"quit\", \"exit\"}:\n",
        "            print(\"\\n>> Bot: See you soon!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize user input and append to conversation history\n",
        "        user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "        messages.append(user_input_ids)\n",
        "\n",
        "        # Generate response and update message history\n",
        "        bot_response_ids = generate_response(model=model, tokenizer=tokenizer, messages=messages)\n",
        "        bot_response = tokenizer.decode(bot_response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"\\n>> Bot: {bot_response}\")\n",
        "        messages.append(bot_response_ids)  # Add bot response to conversation history\n",
        "\n",
        "\n",
        "# Choose and start chatting\n",
        "def prompt_and_chat() -> None:\n",
        "    \"\"\"\n",
        "    Prompt the user to enter a valid dataset ID and execute training on it.\n",
        "    Allows exiting the loop with 'exit', 'quit', or 'q'.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        print(\"\\n> Available datasets:\", \", \".join(DATASETS_ID))\n",
        "        user_input = input(\"  Enter the dataset ID (or type 'q' to exit): \").strip()\n",
        "\n",
        "        if user_input.lower() in {\"q\"}:\n",
        "            print(\"\\n> Process end by user.\")\n",
        "            return\n",
        "\n",
        "        if user_input in DATASETS_ID:\n",
        "            print(f\"\\n> Starting Chat with '{user_input}' bot...\")\n",
        "            model_path = os.path.join(OUTPUT_DIR, user_input)\n",
        "            tokenizer_path = os.path.join(OUTPUT_DIR, user_input)\n",
        "            chatbot_loop(model_path=model_path, tokenizer_path=tokenizer_path)\n",
        "            return\n",
        "\n",
        "        print(f\"\\n> Invalid dataset ID: '{user_input}'. Please choose from: {', '.join(DATASETS_ID)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bPapXgzlhlw"
      },
      "outputs": [],
      "source": [
        "# === Start the Chat === #\n",
        "prompt_and_chat()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}