{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabaudian/Conversational_Agents_project/blob/main/load_and_process_movie_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "S9APbC_znHy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install === #\n",
        "!pip install -q wordcloud\n",
        "!pip install -q contractions\n"
      ],
      "metadata": {
        "id": "F5SMfdIVmrt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXfqhCuCl8m7"
      },
      "outputs": [],
      "source": [
        "# === Import === #\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import html\n",
        "import json\n",
        "import nltk\n",
        "import string\n",
        "import zipfile\n",
        "import requests\n",
        "import warnings\n",
        "import contractions\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.stats import gaussian_kde\n",
        "from typing import Any, Dict, List, Tuple\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Constants & Set-Up === #\n",
        "\n",
        "# Google Drive Paths\n",
        "drive.mount(\"/content/drive\", force_remount=True)                                            # Mount Google Drive\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/Colab_Notebooks\"                                    # Path to Drive main directory\n",
        "\n",
        "# URL\n",
        "URL = \"https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\"   # URL of the dataset\n",
        "\n",
        "# Directory & Paths\n",
        "ROOT_DIR = os.getcwd()                                                                       # Get current working directory (Root)\n",
        "DATA_DIR = os.path.join(DRIVE_BASE_DIR, \"data\")                                              # Path to save data (general directory)\n",
        "CORNELL_DIR = os.path.join(DATA_DIR, \"cornell\")                                              # Path to save Cornell movie-corpus processed data\n",
        "ZIP_PATH = os.path.join(ROOT_DIR, \"movie-corpus.zip\")                                        # Path to the zipped dataset\n",
        "UTTERANCES_PATH = os.path.join(ROOT_DIR, \"movie-corpus\", \"utterances.jsonl\")                 # Path to the utterances file (JSON Lines format)\n",
        "PROCESSED_DATA_PATH = os.path.join(CORNELL_DIR, \"processed_data.csv\")                        # Path to the processed data file (CSV format)\n",
        "\n",
        "# Warnings Set-up\n",
        "warnings.filterwarnings(\"ignore\")                                                            # Suppress warnings for cleaner output during execution\n",
        "\n",
        "# NLTK Downloader\n",
        "nltk.download(\"stopwords\")\n"
      ],
      "metadata": {
        "id": "FxWyP-ogmlR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define Working environment === #\n",
        "def makedirs(dir_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Create a directory if it doesn't exist.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the directory to be created.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"> Directory '{dir_path}' created.\")\n",
        "    else:\n",
        "        print(f\"> Directory '{dir_path}' already exists.\")\n",
        "\n",
        "\n",
        "# Create necessary directory\n",
        "for dir_path in [DATA_DIR, CORNELL_DIR]:\n",
        "    makedirs(dir_path)\n"
      ],
      "metadata": {
        "id": "qzMiyeKtmz_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset"
      ],
      "metadata": {
        "id": "zLDEfufvnR0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "def download_and_extract_dataset(source_url: str, zip_file: str, destination: str) -> None:\n",
        "    \"\"\"\n",
        "    Download the dataset from a source url.\n",
        "\n",
        "    Args:\n",
        "        source_url: url to the data file.\n",
        "        zip_file: path to the compressed file.\n",
        "        destination: where to extract the data.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(zip_file):\n",
        "        response = requests.get(url=source_url)\n",
        "        successful_code = 200\n",
        "\n",
        "        if response.status_code == successful_code:\n",
        "            print(f\"\\n> Downloading data from '{source_url}' to '{destination}'\")\n",
        "            with open(zip_file, mode=\"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"\\n> Failed to download data. Status code: {response.status_code}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Extracts the content of a compressed '.zip' file\n",
        "        with zipfile.ZipFile(zip_file, \"r\") as f:\n",
        "            print(f\"\\n> Unzipping data '{zip_file}' to '{destination}'\")\n",
        "            f.extractall(path=destination)\n",
        "    # Delete the zip file after extraction\n",
        "    os.remove(zip_file)\n"
      ],
      "metadata": {
        "id": "JqoOuzSjnUls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and Unzip the dataset\n",
        "download_and_extract_dataset(source_url=URL, zip_file=ZIP_PATH, destination=ROOT_DIR)\n"
      ],
      "metadata": {
        "id": "nIvNXfvAnXLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief Data Analysis"
      ],
      "metadata": {
        "id": "z7JWnz5W5Lsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'utterance.jsonl' file structure\n",
        "def analyze_utterances(file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Analyzes the structure and content of the 'utterances.jsonl' file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to 'utterances.jsonl' file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n> Analyzing '{file_path}' file...\")\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            first_line = f.readline()\n",
        "\n",
        "            if first_line:\n",
        "                first_entry = json.loads(first_line)\n",
        "\n",
        "                # Detailed insight\n",
        "                for key, value in first_entry.items():\n",
        "                    print(f\"-- Key: '{key}', Value Type: {type(value)}; First entry: {value}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\n> Error: File not found at '{file_path}'\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"\\n> Error decoding JSON: {e}\")\n",
        "\n",
        "\n",
        "# Give a quick look at the data\n",
        "def show_data(file_path: str, n: int = 2) -> None:\n",
        "    \"\"\"\n",
        "    Displays the first lines of the dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to 'utterances.jsonl' file.\n",
        "        n: Number of lines to display.\n",
        "    \"\"\"\n",
        "    print(f\"\\n> First {n} lines of the dataset:\")\n",
        "    with open(file_path, \"rb\") as datafile:\n",
        "        lines = datafile.readlines()\n",
        "\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n"
      ],
      "metadata": {
        "id": "4L_1OuV1nXJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset view\n",
        "analyze_utterances(file_path=UTTERANCES_PATH)\n",
        "show_data(file_path=UTTERANCES_PATH)"
      ],
      "metadata": {
        "id": "_TvqaucXnXHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot text vs word count\n",
        "def plot_text_length_distribution(file_path: str, max_words: int = 100) -> None:\n",
        "    \"\"\"\n",
        "    Plots the distribution of word counts in the utterances.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the utterances.jsonl file.\n",
        "        max_words: Maximum number of words to display on the x-axis.\n",
        "    \"\"\"\n",
        "    word_counts = []\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            text = data.get(\"text\", \"\")\n",
        "            words = text.split()\n",
        "            word_counts.append(len(words))\n",
        "\n",
        "    word_counts = np.array(word_counts)\n",
        "    bins = np.arange(0, max(word_counts) + 1)\n",
        "\n",
        "    # Compute KDE\n",
        "    kde = gaussian_kde(word_counts)\n",
        "    x_kde = np.linspace(0, max(word_counts), 1000)\n",
        "    y_kde = kde(x_kde)\n",
        "\n",
        "    # Histogram\n",
        "    hist_trace = go.Histogram(\n",
        "        x=word_counts,\n",
        "        xbins=dict(start=0, end=max(word_counts), size=1),\n",
        "        name=\"Histogram\",\n",
        "        marker=dict(\n",
        "            color=\"cornflowerblue\",\n",
        "            line=dict(\n",
        "                color=\"black\",\n",
        "                width=1.2\n",
        "            )\n",
        "        ),\n",
        "    opacity=0.85\n",
        "    )\n",
        "\n",
        "    # KDE line\n",
        "    kde_trace = go.Scatter(\n",
        "        x=x_kde,\n",
        "        y=y_kde * len(word_counts),\n",
        "        mode=\"lines\",\n",
        "        name=\"KDE\",\n",
        "        line=dict(color=\"crimson\", width=2)\n",
        "    )\n",
        "\n",
        "    # Plot\n",
        "    fig = go.Figure(data=[hist_trace, kde_trace])\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            \"text\": \"<b>Words Count Distribution Across Utterances in Dataset</b>\",\n",
        "            \"x\": 0.5,\n",
        "            \"xanchor\": \"center\",\n",
        "            \"font\": dict(size=18, family=\"Arial\", color=\"black\", weight=\"bold\"),\n",
        "        },\n",
        "        xaxis_title=\"Number of Words per Utterance\",\n",
        "        yaxis_title=\"Frequency\",\n",
        "        template=\"plotly\",\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        legend=dict(\n",
        "            title=\"Legend\", font=dict(size=12),\n",
        "            bordercolor=\"black\", borderwidth=1\n",
        "        ),\n",
        "    )\n",
        "    fig.update_xaxes(range=[0, max_words])\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "Y7UMufjinXCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text lenght distribution\n",
        "plot_text_length_distribution(file_path=UTTERANCES_PATH)\n"
      ],
      "metadata": {
        "id": "a3vPjUnv0Ie3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation lenght Analysis\n",
        "def plot_conversation_lenght_analysis(file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Generates a scatter plot to analyze the lengths of conversations in the dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the 'utterances.jsonl' file.\n",
        "    \"\"\"\n",
        "    conversation_ids = []\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line_json = json.loads(line)\n",
        "            conversation_ids.append(line_json[\"conversation_id\"])\n",
        "\n",
        "    # Count utterances per conversation\n",
        "    conv_length_counter = Counter(conversation_ids)\n",
        "    conversation_lengths = sorted(conv_length_counter.values())\n",
        "    indices = list(range(len(conversation_lengths)))\n",
        "\n",
        "    # Create DataFrame for plotting\n",
        "    cdf = pd.DataFrame({\n",
        "        \"conversation_index\": indices,\n",
        "        \"conversation_length\": conversation_lengths\n",
        "    })\n",
        "\n",
        "    # Plot\n",
        "    fig = px.scatter(\n",
        "        cdf,\n",
        "        x=\"conversation_index\",\n",
        "        y=\"conversation_length\",\n",
        "        labels={\n",
        "            \"conversation_index\": \"Count\",\n",
        "            \"conversation_length\": \"Lenght\"\n",
        "        },\n",
        "    )\n",
        "    # Layout\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            \"text\": \"<b>Conversation Length Analysis</b>\",\n",
        "            \"x\": 0.5,\n",
        "            \"xanchor\": \"center\"\n",
        "        },\n",
        "        title_font=dict(size=18, family=\"Arial\", color=\"black\"),\n",
        "        xaxis_title_font=dict(size=16, family=\"Arial\", color=\"black\"),\n",
        "        yaxis_title_font=dict(size=16, family=\"Arial\", color=\"black\"),\n",
        "        font=dict(size=14),\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        template=\"plotly\"\n",
        "    )\n",
        "\n",
        "    fig.update_yaxes(matches=None)\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "rUjSuSsY-8tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Conversation Lenght\n",
        "plot_conversation_lenght_analysis(file_path=UTTERANCES_PATH)\n"
      ],
      "metadata": {
        "id": "m7F7kFYT752m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "95h8DiXznVUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse individual utterance lines\n",
        "def parse_line(line_json: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract relevant fields.\n",
        "\n",
        "    Args:\n",
        "        line_json: A dictionary containing the JSON line data.\n",
        "    Returns:\n",
        "        A dictionary containing the extracted fields.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"line_id\": line_json[\"id\"],             # Unique identifier for each utterance\n",
        "        \"character_id\": line_json[\"speaker\"],   # Character who speaks the line\n",
        "        \"text\": line_json[\"text\"],              # Raw text of the utterance\n",
        "    }\n",
        "\n",
        "\n",
        "# Parse the entire dataset into structured format\n",
        "def parse_conversations(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parses a JSONL file of conversations, extracting structured lines and conversation data.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the 'utterances.jsonl' file.\n",
        "\n",
        "    Returns:\n",
        "        The cornell movie dialog individual lines and conversations.\n",
        "    \"\"\"\n",
        "    # ensure the file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "    lines, conversations = {}, {}\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line_json = json.loads(line)\n",
        "            line_obj = parse_line(line_json)\n",
        "            lines[line_obj[\"line_id\"]] = line_obj\n",
        "\n",
        "            conv_id = line_json[\"conversation_id\"]\n",
        "\n",
        "            # Initialize conversation if not seen before\n",
        "            if conv_id not in conversations:\n",
        "                conversations[conv_id] = {\n",
        "                    \"conversation_id\": conv_id,\n",
        "                    \"movie_id\": line_json[\"meta\"][\"movie_id\"],\n",
        "                    \"lines\": []\n",
        "                }\n",
        "            conversations[conv_id][\"lines\"].append(line_obj)\n",
        "\n",
        "    # Ensure each conversation's lines are sorted in order of appearance\n",
        "    for conv in conversations.values():\n",
        "        conv[\"lines\"].sort(key=lambda x: x[\"line_id\"])\n",
        "\n",
        "    return conversations\n",
        "\n",
        "\n",
        "# Clean and normalize dialogue text\n",
        "def clean_dialogue(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans and normalizes dialogue text.\n",
        "\n",
        "    - Handles repeated characters.\n",
        "    - Expands contractions.\n",
        "    - Normalizes punctuation.\n",
        "\n",
        "    Args:\n",
        "        text: Raw dialogue string.\n",
        "\n",
        "    Returns:\n",
        "        Cleaned and normalized text string.\n",
        "    \"\"\"\n",
        "    # Expand contractions (e.g., can't -> cannot)\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # Remove unwanted characters (keep letters, numbers, and basic punctuation)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]\", \"\", text)\n",
        "\n",
        "    # Normalize repeated punctuation and spaces\n",
        "    text = re.sub(r\"\\.{2,}\", \".\", text)         # Convert '...' -> '.'\n",
        "    text = re.sub(r\"(\\!|\\?){2,}\", r\"\\1\", text)  # Convert '!!!' -> '!'\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)         # Remove extra spaces\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Build context-response pairs for model training\n",
        "def create_context_response_dataframe(conversations: dict, context_num: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "      Extract context-response pairs from conversation data and create a DataFrame.\n",
        "      Each row will have a specified number of context columns followed by a response column.\n",
        "\n",
        "      Args:\n",
        "          conversations: A dictionary containing conversations.\n",
        "          context_num: The number of context columns to include in each row.\n",
        "\n",
        "      Returns:\n",
        "          A cleaned DataFrame containing context-response pairs.\n",
        "    \"\"\"\n",
        "    context_response_pairs = []\n",
        "    skipped_lines = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    for conversation in conversations.values():\n",
        "        # Filter and clean valid lines\n",
        "        cleaned_lines = []\n",
        "        for entry in conversation[\"lines\"]:\n",
        "            text = entry.get(\"text\")\n",
        "            total_lines += 1\n",
        "            if text and isinstance(text, str) and text.strip().lower() != \"none\":\n",
        "                cleaned_lines.append(clean_dialogue(text))\n",
        "            else:\n",
        "                skipped_lines += 1\n",
        "\n",
        "        if len(cleaned_lines) > context_num:\n",
        "            for i in range(context_num, len(cleaned_lines)):\n",
        "                context = cleaned_lines[i - context_num:i]\n",
        "                response = cleaned_lines[i]\n",
        "                context_response_pairs.append(context + [response])\n",
        "\n",
        "    # Build context-response df\n",
        "    columns = [\"response\"] + [f\"context_{i}\" for i in range(context_num)]\n",
        "    df = pd.DataFrame(context_response_pairs, columns=columns)\n",
        "    cleaned_df= df.dropna().astype(str) # Drop any lingering NaNs\n",
        "\n",
        "    # Logging\n",
        "    print(f\"\\n - Skipped {skipped_lines} invalid utterances out of {total_lines} total.\")\n",
        "    print(f\"- Dropped {len(df) - len(cleaned_df)} rows with NaNs out of {len(df)} generated pairs.\")\n",
        "    print(f\"- Final dataset size: {len(cleaned_df)} rows\")\n",
        "\n",
        "    # Save to csv\n",
        "    cleaned_df.to_csv(path_or_buf=PROCESSED_DATA_PATH, index=False)\n",
        "\n",
        "    return cleaned_df\n"
      ],
      "metadata": {
        "id": "vFFcSuC5nXrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset\n",
        "def split_dataset(df: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits dataset into train, validation, and test sets with 80/10/10 ratio.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame.\n",
        "        seed: Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing train, validation, and test DataFrames.\n",
        "    \"\"\"\n",
        "    # First split — Train & Temp\n",
        "    train_set, temp_set = train_test_split(df, test_size=0.2, random_state=seed, shuffle=True)\n",
        "    # Split Temp into Validation and Test\n",
        "    valid_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=seed, shuffle=True)\n",
        "\n",
        "    return train_set, valid_set, test_set\n",
        "\n",
        "\n",
        "# Save All Splits to Disk\n",
        "def save_splits(train_set: pd.DataFrame, valid_set: pd.DataFrame, test_set: pd.DataFrame, base_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Save the train, validation, and test sets to disk.\n",
        "\n",
        "    Args:\n",
        "        train_set: Train set DataFrame.\n",
        "        valid_set: Validation set DataFrame.\n",
        "        test_set: Test set DataFrame.\n",
        "        base_dir: Base directory to save the data.\n",
        "    \"\"\"\n",
        "    makedirs(dir_path=base_dir)\n",
        "\n",
        "    train_set.to_csv(os.path.join(base_dir, \"cornell_train.csv\"), index=False)\n",
        "    valid_set.to_csv(os.path.join(base_dir, \"cornell_valid.csv\"), index=False)\n",
        "    test_set.to_csv(os.path.join(base_dir, \"cornell_test.csv\"), index=False)\n",
        "\n",
        "    print(f\"\\n> Data saved to {base_dir}\")\n",
        "\n",
        "\n",
        "# Pre-processing Steps\n",
        "def preprocess_pipeline(utterances_path: str, output_dir: str, context_num: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Dataset Preprocessing pipeline\n",
        "\n",
        "    Args:\n",
        "        utterances_path: Path to the 'utterances.jsonl' file.\n",
        "        output_dir: Directory to save the preprocessed data.\n",
        "        context_num: The number of context columns to include in each row.\n",
        "    \"\"\"\n",
        "    print(\"\\n> Parsing conversations...\")\n",
        "    conversations = parse_conversations(utterances_path)\n",
        "\n",
        "    print(\"\\n> Cleaning and building context-response pairs...\")\n",
        "    df = create_context_response_dataframe(conversations, context_num=context_num)\n",
        "\n",
        "    print(f\"\\n> Dataset size: {len(df)} rows\")\n",
        "    train_set, valid_set, test_set = split_dataset(df)\n",
        "\n",
        "    print(f\"\\n> Train: {len(train_set)} | Val: {len(valid_set)} | Test: {len(test_set)}\")\n",
        "    save_splits(train_set, valid_set, test_set, base_dir=output_dir)\n"
      ],
      "metadata": {
        "id": "jYwUSD-9BFqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Execute Pre-processing === #\n",
        "preprocess_pipeline(utterances_path=UTTERANCES_PATH, output_dir=CORNELL_DIR)\n"
      ],
      "metadata": {
        "id": "e89654OoCZYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Presents processed data\n",
        "processed_df = pd.read_csv(filepath_or_buffer=PROCESSED_DATA_PATH)\n",
        "processed_df.head()\n"
      ],
      "metadata": {
        "id": "fHMrxvdiztW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot WordCloud\n",
        "def plot_wordcloud(csv_path: str, min_sentence_length: int = 3) -> None:\n",
        "    \"\"\"\n",
        "    Processes the 'response' column from a CSV file and generates a WordCloud.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the CSV file containing response-context data.\n",
        "        min_sentence_length: Minimum length of a valid sentence. Defaults to 3.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"The file '{csv_path}' does not exist.\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"response\" not in df.columns:\n",
        "        raise ValueError(\"The CSV file must contain a 'response' column.\")\n",
        "\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    processed_lines = []\n",
        "\n",
        "    for response in df[\"response\"].dropna():\n",
        "        # Remove punctuation and multiple spaces\n",
        "        response = re.sub(r\"[^A-Za-z\\s]\", \" \", response)\n",
        "        response = re.sub(r\"\\s+\", \" \", response).strip()\n",
        "\n",
        "        # Check sentence length and relevance\n",
        "        words = response.split()\n",
        "        if len(words) >= min_sentence_length and any(word.lower() not in stopwords_set for word in words):\n",
        "            processed_lines.append(response)\n",
        "\n",
        "    # Generate and plot WordCloud\n",
        "    text = \" \".join(processed_lines)\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"viridis\",\n",
        "        stopwords=set(stopwords.words(\"english\")),\n",
        "        random_state=42,\n",
        "    ).generate(text.lower())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"WordCloud of Cornell Movie-Corpus\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "YW5sD9tZCr4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plot_wordcloud(csv_path=PROCESSED_DATA_PATH)\n"
      ],
      "metadata": {
        "id": "WthT0aVlCuib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}