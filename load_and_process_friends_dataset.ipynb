{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DWTZvIs0WarT",
        "yYZ2pnmjJJ4v",
        "wXb7g4KESE1Q"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabaudian/Neural_Conversational_Agents_project/blob/main/load_and_process_friends_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "DWTZvIs0WarT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5KKOB0wHut4"
      },
      "outputs": [],
      "source": [
        "# === Install === #\n",
        "!pip install -q wordcloud\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports === #\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from os import listdir\n",
        "from typing import Tuple\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.stats import gaussian_kde\n",
        "from google.colab import drive, files\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "s3nYKTsfIpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Constants & Set-Up === #\n",
        "drive.mount(mountpoint=\"/content/drive\", force_remount=True) # Mount Google Drive\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/Colab_Notebooks\"    # Path to Drive main directory\n",
        "\n",
        "# Kaggle\n",
        "DATASET_ID = \"blessondensil294/friends-tv-series-screenplay-script\" # ID for Kaggle, format: 'dataset_owner_name/dataset_name'\n",
        "\n",
        "# Directory & Paths\n",
        "ROOT_DIR = os.getcwd()\n",
        "DATASET_DIR = os.path.join(ROOT_DIR, \"archive\")                               # Path to dataset directory\n",
        "DATA_DIR = os.path.join(DRIVE_BASE_DIR, \"data\")                               # Path to data directory\n",
        "FRIENDS_DIR = os.path.join(DATA_DIR, \"friends\")                               # Path to friends directory\n",
        "ZIP_PATH = os.path.join(ROOT_DIR, \"friends-tv-series-screenplay-script.zip\")  # Path to the zipped dataset\n",
        "\n",
        "# Regex patterns\n",
        "BRACKET_CONTENT_REGEX = r\"\\(.*\\)\"\n",
        "SPECIAL_CHARACTERS_REGEX = r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\"\n",
        "ALLOWED_CHARACTERS_REGEX = r\"[^.\\',;:?!0-9a-zA-Z \\-]\"\n",
        "PARENTHETICAL_REGEX = r\"\\([^)]*\\)\"\n",
        "EXCLUDE_PATTERNS = [r\".+'s .+\"]\n",
        "\n",
        "# Characters\n",
        "MAIN_CHARACTERS = {\"Rachel\", \"Phoebe\", \"Ross\", \"Chandler\", \"Monica\", \"Joey\"}\n",
        "\n",
        "# Selected characters to form dataset\n",
        "SELESCTED_CHARACTERS = {\"Joey\", \"Phoebe\"}\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download(\"stopwords\")\n"
      ],
      "metadata": {
        "id": "oZiv4hI3IuEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define Working environment === #\n",
        "def makedirs(dir_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Create a directory if it doesn't exist.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the directory to be created.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"> Directory '{dir_path}' created.\")\n",
        "    else:\n",
        "        print(f\"> Directory '{dir_path}' already exists.\")\n",
        "\n",
        "\n",
        "# Create necessary directory\n",
        "for dir_path in [DATA_DIR, FRIENDS_DIR]:\n",
        "    makedirs(dir_path)\n"
      ],
      "metadata": {
        "id": "CFr5vFcFB3iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up Kaggle API and Download the dataset\n",
        "\n",
        "#### Download Kaggle API Credentials:\n",
        "1. Go to your [Kaggle account](https://www.kaggle.com/).\n",
        "2. In the \"API\" section, click on \"Create new API Token\".\n",
        "3. This will download a **kaggle.json** file to your computer.\n",
        "4. Procede with the execution of following code."
      ],
      "metadata": {
        "id": "yYZ2pnmjJJ4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload 'kaggle.json' file\n",
        "files.upload()\n"
      ],
      "metadata": {
        "id": "2UQSkXxTOpEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set-up kaggle\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!pwd\n"
      ],
      "metadata": {
        "id": "1P6B48IhJI4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rw access\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle\n"
      ],
      "metadata": {
        "id": "afmMtyK0JWqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the dataset\n",
        "!kaggle datasets files blessondensil294/friends-tv-series-screenplay-script\n"
      ],
      "metadata": {
        "id": "tu00moPDJVPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d blessondensil294/friends-tv-series-screenplay-script\n"
      ],
      "metadata": {
        "id": "jCP9KNiSJjVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file\n",
        "if os.path.exists(path=ZIP_PATH):\n",
        "  # Unzip the file\n",
        "  with zipfile.ZipFile(file=ZIP_PATH, mode=\"r\") as actors_zip:\n",
        "    actors_zip.extractall(path=DATASET_DIR)\n",
        "    actors_zip.close()\n",
        "\n",
        "  # Delete zip file\n",
        "  os.remove(path=ZIP_PATH)"
      ],
      "metadata": {
        "id": "0f7sA4_gPcoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing the dataset"
      ],
      "metadata": {
        "id": "wXb7g4KESE1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "YF0x0LwtoSvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "def load_friends_dataset(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads the Friends TV Script dataset from the given path.\n",
        "\n",
        "    Args:\n",
        "        path (str): dataset path.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: dataframe.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    # Documents number\n",
        "    docs_num = len(listdir(path))\n",
        "    # Documents name\n",
        "    docs_name = listdir(path)\n",
        "\n",
        "    # Loop over documents\n",
        "    for i in tqdm_notebook(range(docs_num)):\n",
        "        # file name with corresponding episodes link\n",
        "        file_name = docs_name[i]\n",
        "        # Episode ID and Title (e.g.: S01E01 'episode name')\n",
        "        file_label = file_name[:-4]\n",
        "        # Read the documents\n",
        "        with open(os.path.join(path, file_name), encoding=\"utf-8\") as f:\n",
        "            # Loop over lines\n",
        "            for script_line in f.readlines():\n",
        "                row = {\n",
        "                    \"source\": file_label,\n",
        "                    \"line\": script_line,\n",
        "                }\n",
        "                rows.append(row)\n",
        "    # Build dataframe\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "upDzVXJxSKyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw dataset\n",
        "df = load_friends_dataset(path=DATASET_DIR)\n",
        "\n",
        "print(f\"\\n> Dataframe view:\\n {df.head()}\")\n",
        "print(f\"\\n> Dataframe Shape: {df.shape}\")\n"
      ],
      "metadata": {
        "id": "xs1JAO1NSM2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ],
      "metadata": {
        "id": "mzUPASkHoVk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts valid characters\n",
        "def extract_main_characters(character_str: str) -> list:\n",
        "    \"\"\"\n",
        "    Extracts valid main characters from a noisy character string.\n",
        "\n",
        "    Args:\n",
        "        character_str (str): The raw character column string.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of cleaned main character names that match MAIN_CHARACTERS.\n",
        "    \"\"\"\n",
        "    # Remove parenthetical content and normalize casing\n",
        "    cleaned = re.sub(PARENTHETICAL_REGEX, '', character_str, flags=re.IGNORECASE)\n",
        "    cleaned = cleaned.replace(\"&\", \",\").replace(\"/\", \",\").replace(\" and \", \",\").replace(\" to \", \",\")\n",
        "    parts = [part.strip().title() for part in cleaned.split(\",\")]\n",
        "\n",
        "    result = []\n",
        "    for name in parts:\n",
        "        if name in MAIN_CHARACTERS:\n",
        "            result.append(name)\n",
        "        else:\n",
        "            # Check if the part matches an exclusion pattern (e.g., \"Joey's Doctor\")\n",
        "            if any(re.search(pat, name, re.IGNORECASE) for pat in EXCLUDE_PATTERNS):\n",
        "                continue\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Processing the dataset\n",
        "def process_friends_dataset(df: pd.DataFrame, min_line_length: int = 2) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans and processes the dataset containing dialogue lines from the Friends TV show.\n",
        "\n",
        "    This function:\n",
        "    - Removes non-dialogue lines and unwanted formatting\n",
        "    - Extracts only lines spoken by the six main characters\n",
        "    - Handles multiple characters speaking the same line\n",
        "    - Cleans up action or descriptive text in character names\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input dataframe with a 'line' column.\n",
        "        min_line_length (int): Minimum number of characters allowed in a line.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleaned dataframe with 'line' and 'character' columns.\n",
        "    \"\"\"\n",
        "    # Make a copy of the og. dataset\n",
        "    df = df.copy()\n",
        "\n",
        "    # Remove lines that start with brackets\n",
        "    df = df[~df[\"line\"].str.startswith((\"[\", \"(\"))]\n",
        "\n",
        "    # Strip whitespace\n",
        "    df[\"line\"] = df[\"line\"].str.strip()\n",
        "\n",
        "    # Remove parenthetical content from line\n",
        "    df[\"line\"] = df[\"line\"].str.replace(BRACKET_CONTENT_REGEX, \"\", regex=True)\n",
        "\n",
        "    # Replace special characters and clean up\n",
        "    df[\"line\"] = df[\"line\"].str.replace(SPECIAL_CHARACTERS_REGEX, \" \", regex=True)\n",
        "    df[\"line\"] = df[\"line\"].str.replace(ALLOWED_CHARACTERS_REGEX, \"\", regex=True)\n",
        "\n",
        "    # Remove empty or NaN lines\n",
        "    df = df[df[\"line\"].notna() & df[\"line\"].str.strip().ne(\"\")]\n",
        "\n",
        "    # Split into character and actual spoken line\n",
        "    df[[\"character_raw\", \"line\"]] = df[\"line\"].str.split(\":\", n=1, expand=True)\n",
        "\n",
        "    # Further strip and filter\n",
        "    df.replace(r\"^\\s*$\", float(\"NaN\"), regex=True, inplace=True)\n",
        "    df.dropna(subset=[\"character_raw\", \"line\"], inplace=True)\n",
        "    df[\"character_raw\"] = df[\"character_raw\"].astype(str).str.strip()\n",
        "    df[\"line\"] = df[\"line\"].astype(str).str.strip()\n",
        "\n",
        "    df = df[df[\"line\"].str.len() >= min_line_length]\n",
        "\n",
        "    # Extract main characters\n",
        "    df[\"character\"] = df[\"character_raw\"].apply(extract_main_characters)\n",
        "    # Drop lines that have no valid character\n",
        "    df = df[df[\"character\"].map(len) > 0]\n",
        "    # Explode into one row per character\n",
        "    df = df.explode(\"character\")\n",
        "\n",
        "    # Normalize text for tone modeling\n",
        "    df[\"line\"] = df[\"line\"].str.replace(r\"[.!?]{2,}\", \".\", regex=True)  # clean repeated punctuation\n",
        "    df[\"line\"] = df[\"line\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()  # clean whitespace\n",
        "\n",
        "    # Final dataframe with 'line' first and 'character' second\n",
        "    df = df[[\"line\", \"character\"]].reset_index(drop=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = os.path.join(FRIENDS_DIR, \"friends_dataset.csv\")\n",
        "    df.to_csv(path_or_buf=csv_path, index=False)\n",
        "    print(f\"> Processed Dataset saved at: {csv_path}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "KWMlqBsGSVAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing the dataset\n",
        "processed_df = process_friends_dataset(df=df)\n",
        "print(f\"\\n> Processed Dataset View:\\n {processed_df.head()}\")\n"
      ],
      "metadata": {
        "id": "OIE7UoB6HDSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot text vs word count\n",
        "def plot_text_length_distribution_friends(df: pd.DataFrame, max_words: int = 100) -> None:\n",
        "    \"\"\"\n",
        "    Plots the distribution and KDE of word counts in the Friends dialogue dataset using Plotly.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Processed DataFrame with a 'line' column.\n",
        "        max_words (int): Maximum word count to display on the x-axis.\n",
        "    \"\"\"\n",
        "    # Calculate word counts\n",
        "    word_counts = df[\"line\"].apply(lambda x: len(str(x).split())).tolist()\n",
        "    bins = np.arange(0, max(word_counts) + 1)\n",
        "\n",
        "    # KDE estimate\n",
        "    kde = gaussian_kde(word_counts)\n",
        "    x_kde = np.linspace(0, max(word_counts), 1000)\n",
        "    y_kde = kde(x_kde)\n",
        "\n",
        "    # Build Plotly figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add histogram\n",
        "    fig.add_trace(go.Histogram(\n",
        "        x=word_counts,\n",
        "        xbins=dict(start=0, end=max(word_counts), size=1),\n",
        "        name=\"Histogram\",\n",
        "        marker=dict(\n",
        "            color=\"cornflowerblue\",\n",
        "            line=dict(\n",
        "                color=\"black\",\n",
        "                width=1.2\n",
        "            )\n",
        "        ),\n",
        "    opacity=0.85\n",
        "    ))\n",
        "\n",
        "    # Add KDE line\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_kde,\n",
        "        y=y_kde * len(word_counts),  # Scale KDE to match histogram frequency\n",
        "        mode=\"lines\",\n",
        "        name=\"KDE\",\n",
        "        line=dict(color=\"crimson\", width=2),\n",
        "    ))\n",
        "\n",
        "    # Layout\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            \"text\": \"<b>Distribution of Word Counts per Utterance</b>\",\n",
        "            \"x\": 0.5,\n",
        "            \"xanchor\": \"center\",\n",
        "            \"font\": dict(size=18, family=\"Arial\", color=\"black\", weight=\"bold\"),\n",
        "        },\n",
        "        xaxis_title=\"Number of Words per Utterance\",\n",
        "        yaxis_title=\"Frequency\",\n",
        "        template=\"plotly\",\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        legend=dict(\n",
        "            title=\"Legend\", font=dict(size=12),\n",
        "            bordercolor=\"black\", borderwidth=1\n",
        "        ),\n",
        "    )\n",
        "    fig.update_xaxes(range=[0, max_words])\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "m7xxt64vF5VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text lenght distribution\n",
        "plot_text_length_distribution_friends(df=processed_df)\n"
      ],
      "metadata": {
        "id": "4lzN4jGhF56s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Character Appearances"
      ],
      "metadata": {
        "id": "WngFIfOtf33m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character Appearances\n",
        "def plot_count_character_appearances(df: pd.DataFrame, main_characters: set) -> None:\n",
        "    \"\"\"\n",
        "    Counts the number of times each main character appears in the 'character' column\n",
        "    and plots a bar graph with the count of appearances for each character.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing the character data.\n",
        "        main_characters (set): A set of main character names to search in the dataframe.\n",
        "    \"\"\"\n",
        "    # Colors\n",
        "    colors = [\"#FF5733\", \"#33FF57\", \"#3357FF\", \"#FF33A8\", \"#FFC300\", \"#8D33FF\"]\n",
        "\n",
        "    # Dictionary to store counts\n",
        "    character_counts = {character: 0 for character in main_characters}\n",
        "\n",
        "    # Loop over each character and count the appearances\n",
        "    for character in main_characters:\n",
        "        # Count occurrences of the character in the 'character' column\n",
        "        count = df[df[\"character\"].str.contains(character, case=False, na=False)].shape[0]\n",
        "        character_counts[character] = count\n",
        "\n",
        "    # Plotting the bar graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(character_counts.keys(), character_counts.values(), color=colors, edgecolor=\"black\", linewidth=0.7)\n",
        "\n",
        "    # Annotate each bar with its count value\n",
        "    for bar in bars:\n",
        "        y_val = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, y_val, str(y_val), ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.xlabel(\"Character\", fontsize=12)\n",
        "    plt.ylabel(\"Count of Appearances\", fontsize=12)\n",
        "    plt.title(\"Character Appearances in the Dataset\", fontsize=14,  fontweight=\"bold\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2RqyEyr3SZWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot character appearances\n",
        "plot_count_character_appearances(df=processed_df, main_characters=MAIN_CHARACTERS)\n"
      ],
      "metadata": {
        "id": "J2RlcTK3S5lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define dataset structure"
      ],
      "metadata": {
        "id": "-MJ3-NT2gBUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the response-context structure\n",
        "def generate_character_context_csv(df: pd.DataFrame, character: str, context_size: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the dataset for a selected character, structures it into response-context format,\n",
        "    and saves it as a CSV file in a directory named after the selected character.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Processed dataframe containing dialogue data.\n",
        "        character (str): The character whose dialogues should be extracted.\n",
        "        context_size (int, optional): The number of previous lines to include as context. Defaults to 5.\n",
        "    Returns:\n",
        "        pd.DataFrame: A processed dataframe containing dialogue data.\n",
        "    \"\"\"\n",
        "    # Ensure character is in the main characters set\n",
        "    if character not in MAIN_CHARACTERS:\n",
        "        raise ValueError(f\"Character '{character}' is not in the main character list: {MAIN_CHARACTERS}\")\n",
        "\n",
        "    # Get indices where the character speaks\n",
        "    idx_character = df[df[\"character\"] == character].index\n",
        "\n",
        "    # Prepare rows for the new dataframe\n",
        "    dataframe_rows = []\n",
        "    columns = [\"response\"] + [f\"context_{i}\" for i in range(context_size)]\n",
        "\n",
        "    for i in idx_character:\n",
        "        row = [df[\"line\"][i]]\n",
        "\n",
        "        for j in range(0, context_size):\n",
        "            line_idx = max(i - j - 1, 0)  # Ensure valid index\n",
        "            row.append(df[\"line\"][line_idx])\n",
        "\n",
        "        dataframe_rows.append(row)\n",
        "\n",
        "    # Build the new dataframe\n",
        "    df = pd.DataFrame(dataframe_rows, columns=columns)\n",
        "\n",
        "    # Save to csv\n",
        "    character_dir = os.path.join(FRIENDS_DIR, character) # Build character's folder\n",
        "    makedirs(dir_path=character_dir)\n",
        "\n",
        "    csv_path = os.path.join(character_dir, f\"{character}.csv\") #\n",
        "    df.to_csv(path_or_buf=csv_path, index=False)\n",
        "\n",
        "    print(f\" > CSV file saved at: {csv_path}\\n\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "N-URzJlETAjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response/Context data structure\n",
        "for character in SELESCTED_CHARACTERS:\n",
        "    print(f\"> Define dataset for {character}...\\n\")\n",
        "    context_df = generate_character_context_csv(df=processed_df, character=character)\n",
        "    print(f\"  > Context Dataset shape: {context_df.shape}\\n\")\n"
      ],
      "metadata": {
        "id": "eFSXlXAyTDZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"> Joey dataset View: \\n\")\n",
        "df = pd.read_csv(os.path.join(FRIENDS_DIR, \"Joey\", \"Joey.csv\"))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "6iGdMzrV2Adz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slit Dataset\n",
        "def split_dataset(df: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits dataset into train, validation, and test sets with 80/10/10 ratio.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame.\n",
        "        seed: Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing train, validation, and test DataFrames.\n",
        "    \"\"\"\n",
        "    # First split — Train & Temp (Val + Test)\n",
        "    train_set, temp_set = train_test_split(df, test_size=0.2, random_state=seed, shuffle=True)\n",
        "    # Split Temp into Validation and Test\n",
        "    valid_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=seed, shuffle=True)\n",
        "\n",
        "    return train_set, valid_set, test_set\n",
        "\n",
        "\n",
        "# Save All Splits to Disk\n",
        "def save_splits(character:str, train_set, valid_set, test_set, base_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Save the train, validation, and test sets to disk.\n",
        "\n",
        "    Args:\n",
        "        character: Character name.\n",
        "        train_set: Train set DataFrame.\n",
        "        valid_set: Validation set DataFrame.\n",
        "        test_set: Test set DataFrame.\n",
        "        base_dir: Base directory to save the data.\n",
        "    \"\"\"\n",
        "    train_set.to_csv(os.path.join(base_dir, character, f\"{character}_train.csv\"), index=False)\n",
        "    valid_set.to_csv(os.path.join(base_dir, character, f\"{character}_valid.csv\"), index=False)\n",
        "    test_set.to_csv(os.path.join(base_dir, character, f\"{character}_test.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "OFSl45LU-oHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset and Save to Disk\n",
        "for character in SELESCTED_CHARACTERS:\n",
        "  print(f\"\\n> Split dataset for {character}:\")\n",
        "  character_df = pd.read_csv(os.path.join(FRIENDS_DIR, character, f\"{character}.csv\"))\n",
        "\n",
        "  train_set, valid_set, test_set = split_dataset(df=character_df)\n",
        "\n",
        "  save_splits(\n",
        "      character=character,\n",
        "      train_set=train_set,\n",
        "      valid_set=valid_set,\n",
        "      test_set=test_set,\n",
        "      base_dir=FRIENDS_DIR\n",
        "  )\n",
        "\n",
        "  print(f\"  > Train Dataset shape: {train_set.shape}\")\n",
        "  print(f\"  > Validation Dataset shape: {valid_set.shape}\")\n",
        "  print(f\"  > Test Dataset shape: {test_set.shape}\")\n"
      ],
      "metadata": {
        "id": "jzXGFBsE_IZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot WordColud"
      ],
      "metadata": {
        "id": "43dVTSxEgH0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud\n",
        "def plot_wordcloud(csv_path: str, min_sentence_length: int = 10) -> None:\n",
        "    \"\"\"\n",
        "    Processes the 'response' column from a CSV file and generates a WordCloud.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the CSV file containing response-context data.\n",
        "        min_sentence_length (int, optional): Minimum length of a valid sentence. Defaults to 3.\n",
        "    Returns: None.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"The file '{csv_path}' does not exist.\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"response\" not in df.columns:\n",
        "        raise ValueError(\"The CSV file must contain a 'response' column.\")\n",
        "\n",
        "    # Extract character name from filename (removes directory path and extension)\n",
        "    character = os.path.splitext(os.path.basename(csv_path))[0]\n",
        "\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    processed_lines = []\n",
        "\n",
        "    for response in df[\"response\"].dropna():\n",
        "        # Remove punctuation and multiple spaces\n",
        "        response = re.sub(r\"[^A-Za-z\\s]\", \" \", response)\n",
        "        response = re.sub(r\"\\s+\", \" \", response).strip()\n",
        "\n",
        "        # Check sentence length and relevance\n",
        "        words = response.split()\n",
        "        if len(words) >= min_sentence_length and any(word.lower() not in stopwords_set for word in words):\n",
        "            processed_lines.append(response)\n",
        "\n",
        "    # Generate and plot WordCloud\n",
        "    text = \" \".join(processed_lines)\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"viridis\",\n",
        "        stopwords=set(stopwords.words(\"english\")),\n",
        "        random_state=42,\n",
        "    ).generate(text.lower())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"WordCloud of {character}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3TjYb23JV7_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud associated with the selected character\n",
        "for selected_character in SELESCTED_CHARACTERS:\n",
        "  print(f\"\\n> Plot WordCloud for {selected_character}:\\n\")\n",
        "  csv_file_path = os.path.join(FRIENDS_DIR, selected_character, f\"{selected_character}.csv\")\n",
        "  plot_wordcloud(csv_path=csv_file_path)\n"
      ],
      "metadata": {
        "id": "i_AXgiYEV_Cg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}